# -*- coding: utf-8 -*-
"""ARIMA and Hybrid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fFlcrokWlrPsiohq_2Bn5_-IjuJO3BxB
"""



!pip install pmdarima

import pandas as pd
import numpy as np

df= pd.read_csv("data.csv",sep=";",index_col="Date",parse_dates=True)

df.head()

df= df[["Case"]]

df["Case"]=df["Case"].diff()
df["Case"].fillna(1,inplace=True)
df["Case"]=df["Case"].astype(int)

df.head()

from statsmodels.tsa.seasonal import seasonal_decompose

decomposition = seasonal_decompose(df['Case'], model='additive')

# Plot the decomposed components
decomposition.plot().suptitle('Time Series Decomposition of Case Data')

residuals = decomposition.resid.dropna()  # Removing NaN values which are a result of the decomposition
z_scores = np.abs((residuals - residuals.mean()) / residuals.std())

threshold = 3
anomalies = residuals[z_scores > threshold]

# Replace anomalies with the median value of 'Case'
median_value = df['Case'].median()
df.loc[anomalies.index, 'Case'] = median_value

df.plot(figsize=(12,6))

from pmdarima import auto_arima
import warnings
warnings.filterwarnings("ignore")

stepwise_fit = auto_arima(df,trace=True,
                          supress_warnings=True)

stepwise_fit.summary()

from statsmodels.tsa.arima_model import ARIMA

from statsmodels.tsa.arima.model import ARIMA

print(df.shape)
train= df.iloc[:-7]
test= df.iloc[-7:]
print(train.shape,test.shape)

print(train)

model= ARIMA(train,order=(5,1,5))
model=model.fit()
model.summary()

start= len(train)
end=len(train)+len(test)-1
pred=model.predict(start=start,end=end,typ="levels")
print(pred)

pred.index= df.index[start:end+1]
print(pred)

dataframe = pred.to_frame(name='pred_case')

print(dataframe)

pred= dataframe

import matplotlib.pyplot as plt
pred["pred_case"].plot(figsize=(14,5),fontsize=14,label="Forecast")
test["Case"].plot(figsize=(14,5),title= "Actual vs Forecast Values",fontsize=14,label="Test")
plt.legend()
plt.ylim(500, 1400)
plt.grid()
plt.show()

from sklearn.metrics import mean_squared_error
from math import sqrt
rmse= sqrt(mean_squared_error(pred,test["Case"]))
print(rmse)

from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error

# Calculating MAE, MSE, and MAPE between test and hybrid columns
mae = mean_absolute_error(pred["pred_case"],test["Case"])
mse = mean_squared_error(pred["pred_case"],test["Case"])
mape = mean_absolute_percentage_error(pred["pred_case"],test["Case"])

mae, mse, mape

"""burası train kısmı"""

start= len(train)
end=len(train)+len(test)-1
pred_train=model.predict(0,end=start-1,typ="levels")
print(pred_train)

pred_train.plot(label="Forecast",color="red")
train["Case"].plot(color="black")
plt.grid()
plt.legend()
plt.show()

res_train = train["Case"]- pred_train
print(res_train)

res_test = test["Case"]- dataframe["pred_case"]
print(res_test)



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

res_train= res_train.to_frame()

res_test= res_test.to_frame()

print(res_test)

print(res_train)

type(res_train)

res_test = res_test.rename(columns={0:"residual"})

print(res_train)
print(type(res_train))
print(res_train.info())

res_train = res_train.rename(columns={0:"residual"})

print(res_train)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(-1, 1))

scaler.fit(res_train)
scaled_train = scaler.transform(res_train)
scaled_test= scaler.transform(res_test)

scaled_test

from keras.preprocessing.sequence import TimeseriesGenerator

n_input = 3
n_features = 1
generator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)

X,y = generator[0]
print(f'Given the Array: \n{X.flatten()}')
print(f'Predict this y: \n {y}')





n_input = 7
generator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=64)

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

model = Sequential()
model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

model.summary()

model.fit(generator,epochs=30)

loss_per_epoch = model.history.history['loss']
plt.plot(range(len(loss_per_epoch)),loss_per_epoch)

last_train_batch = scaled_train[-7:]

last_train_batch = last_train_batch.reshape((1, n_input, n_features))

model.predict(last_train_batch)

scaled_test[0]

test_predictions = []

first_eval_batch = scaled_train[-n_input:]
current_batch = first_eval_batch.reshape((1, n_input, n_features))

for i in range(len(res_test)):

    # get the prediction value for the first batch
    current_pred = model.predict(current_batch)[0]

    # append the prediction into the array
    test_predictions.append(current_pred)

    # use the prediction to update the batch and remove the first value
    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)



test_predictions



true_predictions = scaler.inverse_transform(test_predictions)
print(true_predictions)

pred

pred["pred_case"]

pred["lstm_error_forecast"]=true_predictions

pred

pred["test"]=test

print(pred)

data = {
    "Date": ["2022-05-25", "2022-05-26", "2022-05-27", "2022-05-28", "2022-05-29", "2022-05-30", "2022-05-31"],
    "pred_case": [1212.875463, 938.284464, 1131.413035, 1214.140892, 870.095033, 879.248305, 1241.022172],
    "lstm_error_forecast": [-20.381680,-14.187756,-16.582858,51.997128,-73.344073,-7.689943,-11.949774],
    "test": [1260.0, 1310.0, 940.0, 966.0, 864.0, 908.0, 975.0]
}

# Convert to DataFrame
df = pd.DataFrame(data)
df['Date'] = pd.to_datetime(df['Date'])  # Converting the Date column to datetime
df.set_index('Date', inplace=True)  # Setting the Date column as the index

# Calculate the hybrid column
df['hybrid'] = df['pred_case'] + df['lstm_error_forecast']

df

newdf = df[["test","hybrid"]]

newdf

newdf.plot(figsize=(14,5))
plt.ylim(500, 1400)
plt.grid()
plt.show()

from sklearn.metrics import mean_squared_error
from math import sqrt
rmse=sqrt(mean_squared_error(newdf['test'],newdf['hybrid']))
print(rmse)

from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error

# Calculating MAE, MSE, and MAPE between test and hybrid columns
mae = mean_absolute_error(df['test'], df['hybrid'])
mse = mean_squared_error(df['test'], df['hybrid'])
mape = mean_absolute_percentage_error(df['test'], df['hybrid'])

mae, mse, mape